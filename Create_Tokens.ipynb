{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd1d8ee3-4999-4acd-978c-66ce87c3044f",
   "metadata": {},
   "source": [
    "### Step 1: Creating Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0069ebde-8176-4e64-89ab-b0ca330d22e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of raw text 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"the-verdict.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    raw_text=f.read()\n",
    "\n",
    "print(\"length of raw text\",len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53672f29-75e0-4954-b084-b85400a0dd90",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The print command prints the total number of characters followed by the first 100\n",
    "characters of this file for illustration purposes. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12514f89-298e-4a80-96a1-057a160b413a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Note that it's common to process millions of articles and hundreds of thousands of\n",
    "books -- many gigabytes of text -- when working with LLMs. However, for educational\n",
    "purposes, it's sufficient to work with smaller text samples like a single book to\n",
    "illustrate the main ideas behind the text processing steps and to make it possible to\n",
    "run it in reasonable time on consumer hardware. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ec10cb-eae3-4a39-8114-e14b22b4762b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "How can we best split this text to obtain a list of tokens? For this, we go on a small\n",
    "excursion and use Python's regular expression library re for illustration purposes. (Note\n",
    "that you don't have to learn or memorize any regular expression syntax since we will\n",
    "transition to a pre-built tokenizer later in this chapter.) </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "424d5a17-056a-4e8b-b1dd-c99f367d49a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', ' ', 'HAD', ' ', 'always', ' ', 'thought', ' ', 'Jack', ' ', 'Gisburn', ' ', 'rather', ' ', 'a', ' ', 'cheap', ' ', 'genius', '--']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "print(result[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49f950bd-f503-490a-aed1-a6d139afed1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "new_result=[]\n",
    "for item in result:\n",
    "    if(item.strip()):\n",
    "        new_result.append(item)\n",
    "result=new_result\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e057ede-881f-4f23-9aa9-290244682277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "all_word=sorted(set(result))\n",
    "print(len(all_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f53b7c1-0c11-42b4-a3d0-e7f557c8c661",
   "metadata": {},
   "source": [
    "## Step 2: Creating Token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9f6a99d-fbb1-4aa4-8740-489b0c35b6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab={}\n",
    "reverse_vocab={}\n",
    "i=0\n",
    "for item in all_word:\n",
    "    vocab[item]=i\n",
    "    reverse_vocab[i]=item\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a8d6ba-fbbe-4964-8cb8-7a1f50c12d0a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "As we can see, based on the output above, the dictionary contains individual tokens\n",
    "associated with unique integer labels. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa4498a-b2e2-416f-bdee-d245ae195a2e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "After determining that the vocabulary size is 1,130 via the above code, we create the\n",
    "vocabulary and print its first 51 entries for illustration purposes:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421a8d26-5a17-4530-ae11-f7c7b9b32bce",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "We can modify the tokenizer to use an <|unk|> token if it\n",
    "encounters a word that is not part of the vocabulary. \n",
    "\n",
    "Furthermore, we add a token between\n",
    "unrelated texts. \n",
    "\n",
    "For example, when training GPT-like LLMs on multiple independent\n",
    "documents or books, it is common to insert a token before each document or book that\n",
    "follows a previous text source\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f0a002-62eb-4cf9-be1d-770d15f856c4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's now modify the vocabulary to include these two special tokens, <unk> and\n",
    "<|endoftext|>, by adding these to the list of all unique words that we created in the\n",
    "previous section:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c585c42-b030-4c25-979a-822be0bb6c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens=sorted(set(vocab))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab={}\n",
    "i=0\n",
    "for item in all_tokens:\n",
    "    vocab[item]=i\n",
    "    reverse_vocab[i]=item\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45177cf-00e7-45ac-8b30-4c9e60173b8c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Step 1: Store the vocabulary as a class attribute for access in the encode and decode methods\n",
    "    \n",
    "Step 2: Create an inverse vocabulary that maps token IDs back to the original text tokens\n",
    "\n",
    "Step 3: Process input text into token IDs\n",
    "\n",
    "Step 4: Convert token IDs back into text\n",
    "\n",
    "Step 5: Replace spaces before the specified punctuation\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a40e9c80-843c-455a-ac85-603c6c528835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode (text,vocab):\n",
    "    tokens=re.split(r'([,.:;?_!\"()\\']|--|\\s)',text)\n",
    "    idx=[]\n",
    "    for item in tokens:\n",
    "        if item.strip():\n",
    "            idx.append(item)\n",
    "    tokens=idx\n",
    "    idx=[]\n",
    "    for token in tokens:\n",
    "        if token in vocab:\n",
    "            idx.append(vocab[token])\n",
    "        else:\n",
    "            idx.append(vocab[\"<|unk|>\"])\n",
    "    return idx\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "848f055e-943d-4a0b-8596-a83e75af2b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(idx,reverse_vocab):\n",
    "    words=[]\n",
    "    for i in idx:\n",
    "        if i in reverse_vocab:\n",
    "            words.append(reverse_vocab[i])\n",
    "    return \" \".join(words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7273088-e09e-4850-8d1c-d3e28b57809c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 6, 549, 602, 25, 1131, 1059, 722, 697, 769]\n"
     ]
    }
   ],
   "source": [
    "text1=\"Thwing--his last Chicago sitter\"\n",
    "text2=\"value of my picture\"\n",
    "text=\"<|endoftext|> \".join([text1,text2])\n",
    "idx=encode(text,vocab)\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2726a415-40b6-47f5-af25-338b8b2c846d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thwing -- his last Chicago <|unk|> value of my picture\n"
     ]
    }
   ],
   "source": [
    "decoded_text=decode(idx,reverse_vocab)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c8d624-4bd0-492e-b0f7-202ce876cadc",
   "metadata": {},
   "source": [
    "### BYTE PAIR ENCODING (BPE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c807bd-39f9-4788-90ec-ff89080b704c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "We implemented a simple tokenization scheme in the previous sections for illustration\n",
    "purposes. \n",
    "\n",
    "This section covers a more sophisticated tokenization scheme based on a concept\n",
    "called byte pair encoding (BPE). \n",
    "\n",
    "The BPE tokenizer covered in this section was used to train\n",
    "LLMs such as GPT-2, GPT-3, and the original model used in ChatGPT.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb05aa6-b974-4d77-a093-93a72787b02b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Since implementing BPE can be relatively complicated, we will use an existing Python\n",
    "open-source library called tiktoken (https://github.com/openai/tiktoken). \n",
    "\n",
    "This library implements\n",
    "the BPE algorithm very efficiently based on source code in Rust.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fd91ff4-e51b-4d77-adb8-ccebde8e79ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tiktoken in c:\\users\\user\\appdata\\roaming\\python\\python313\\site-packages (0.12.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\user\\appdata\\roaming\\python\\python313\\site-packages (from tiktoken) (2025.10.23)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\user\\appdata\\roaming\\python\\python313\\site-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\appdata\\roaming\\python\\python313\\site-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\roaming\\python\\python313\\site-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\roaming\\python\\python313\\site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\roaming\\python\\python313\\site-packages (from requests>=2.26.0->tiktoken) (2025.10.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2eba8ea0-91a5-456f-a393-57f3ed68c8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12.0\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "print(tiktoken.__version__)\n",
    "tokenizer=tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fa37a0-d3e5-4c75-a5d6-3e5fbf1f3d72",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "The code above prints the following token IDs:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f479f89-27e8-4a78-ba25-e63b6f553e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text=( \n",
    "     \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "values=tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c53b7f-2a64-4c1f-aa8e-8afe517dcaac",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "The algorithm underlying BPE breaks down words that aren't in its predefined vocabulary\n",
    "into smaller subword units or even individual characters.\n",
    "\n",
    "The enables it to handle out-ofvocabulary words. \n",
    "\n",
    "So, thanks to the BPE algorithm, if the tokenizer encounters an\n",
    "unfamiliar word during tokenization, it can represent it as a sequence of subword tokens or\n",
    "characters\n",
    "    \n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d31bbdde-7e32-4a14-88c6-093dd2b5a741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31373, 1312, 716, 28509, 312]\n",
      "hello i am rashid\n"
     ]
    }
   ],
   "source": [
    "values=tokenizer.encode(\"hello i am rashid\")\n",
    "print(values)\n",
    "strings=tokenizer.decode(values)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d8407e-587f-4b3a-9b41-9963aac18cf9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Executing the code above will return 5145, the total number of tokens in the training set,\n",
    "after applying the BPE tokenizer.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53bf4c9c-4cc1-4b96-a901-1b48a92cb04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40, 367, 2885, 1464, 1807, 3619, 402, 271, 10899, 2138, 257, 7026, 15632, 438, 2016, 257, 922, 5891, 1576, 438]\n",
      "5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    raw_data=f.read()\n",
    "enc_text=tokenizer.encode(raw_data)\n",
    "print(enc_text[:20])\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15052e49-bf2d-4529-99b2-72959c4cea77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40, 367, 2885, 1464]\n",
      "[367, 2885, 1464, 1807]\n"
     ]
    }
   ],
   "source": [
    "context_size=4;\n",
    "x=enc_text[:context_size]\n",
    "y=enc_text[1:context_size+1]\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba54a6b8-a30c-4dff-97a6-e13adc7c58fb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Processing the inputs along with the targets, which are the inputs shifted by one position,\n",
    "we can then create the next-word prediction tasks as\n",
    "follows:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9cbdb623-1313-4e30-b40c-1f296954f93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40] ------> 367\n",
      "[40, 367] ------> 2885\n",
      "[40, 367, 2885] ------> 1464\n",
      "[40, 367, 2885, 1464] ------> 1807\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,context_size+1):\n",
    "    context=enc_text[:i]\n",
    "    desired=enc_text[i]\n",
    "    print(context,\"------>\",desired)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ef0f5f-d58d-41cd-b7a5-a9108a409a79",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Everything left of the arrow (---->) refers to the input an LLM would receive, and the token\n",
    "ID on the right side of the arrow represents the target token ID that the LLM is supposed to\n",
    "predict.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9050941e-9161-41a8-9ba6-3e623343a7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I ------>  H\n",
      "I H ------> AD\n",
      "I HAD ------>  always\n",
      "I HAD always ------>  thought\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,context_size+1):\n",
    "    context=enc_text[:i]\n",
    "    desired=enc_text[i]\n",
    "    print(tokenizer.decode(context),\"------>\",tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc98a5c7-8fe0-4837-8936-4e67f54e5a83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
