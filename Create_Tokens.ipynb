{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd1d8ee3-4999-4acd-978c-66ce87c3044f",
   "metadata": {},
   "source": [
    "### Step 1: Creating Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0069ebde-8176-4e64-89ab-b0ca330d22e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of raw text 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"the-verdict.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    raw_text=f.read()\n",
    "\n",
    "print(\"length of raw text\",len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53672f29-75e0-4954-b084-b85400a0dd90",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The print command prints the total number of characters followed by the first 100\n",
    "characters of this file for illustration purposes. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12514f89-298e-4a80-96a1-057a160b413a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Note that it's common to process millions of articles and hundreds of thousands of\n",
    "books -- many gigabytes of text -- when working with LLMs. However, for educational\n",
    "purposes, it's sufficient to work with smaller text samples like a single book to\n",
    "illustrate the main ideas behind the text processing steps and to make it possible to\n",
    "run it in reasonable time on consumer hardware. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ec10cb-eae3-4a39-8114-e14b22b4762b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "How can we best split this text to obtain a list of tokens? For this, we go on a small\n",
    "excursion and use Python's regular expression library re for illustration purposes. (Note\n",
    "that you don't have to learn or memorize any regular expression syntax since we will\n",
    "transition to a pre-built tokenizer later in this chapter.) </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "424d5a17-056a-4e8b-b1dd-c99f367d49a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', ' ', 'HAD', ' ', 'always', ' ', 'thought', ' ', 'Jack', ' ', 'Gisburn', ' ', 'rather', ' ', 'a', ' ', 'cheap', ' ', 'genius', '--']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "print(result[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49f950bd-f503-490a-aed1-a6d139afed1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "new_result=[]\n",
    "for item in result:\n",
    "    if(item.strip()):\n",
    "        new_result.append(item)\n",
    "result=new_result\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e057ede-881f-4f23-9aa9-290244682277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "all_word=sorted(set(result))\n",
    "print(len(all_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f53b7c1-0c11-42b4-a3d0-e7f557c8c661",
   "metadata": {},
   "source": [
    "## Step 2: Creating Token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9f6a99d-fbb1-4aa4-8740-489b0c35b6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab={}\n",
    "reverse_vocab={}\n",
    "i=0\n",
    "for item in all_word:\n",
    "    vocab[item]=i\n",
    "    reverse_vocab[i]=item\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a8d6ba-fbbe-4964-8cb8-7a1f50c12d0a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "As we can see, based on the output above, the dictionary contains individual tokens\n",
    "associated with unique integer labels. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa4498a-b2e2-416f-bdee-d245ae195a2e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "After determining that the vocabulary size is 1,130 via the above code, we create the\n",
    "vocabulary and print its first 51 entries for illustration purposes:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421a8d26-5a17-4530-ae11-f7c7b9b32bce",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "We can modify the tokenizer to use an <|unk|> token if it\n",
    "encounters a word that is not part of the vocabulary. \n",
    "\n",
    "Furthermore, we add a token between\n",
    "unrelated texts. \n",
    "\n",
    "For example, when training GPT-like LLMs on multiple independent\n",
    "documents or books, it is common to insert a token before each document or book that\n",
    "follows a previous text source\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f0a002-62eb-4cf9-be1d-770d15f856c4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's now modify the vocabulary to include these two special tokens, <unk> and\n",
    "<|endoftext|>, by adding these to the list of all unique words that we created in the\n",
    "previous section:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c585c42-b030-4c25-979a-822be0bb6c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens=sorted(set(vocab))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab={}\n",
    "i=0\n",
    "for item in all_tokens:\n",
    "    vocab[item]=i\n",
    "    reverse_vocab[i]=item\n",
    "    i+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
